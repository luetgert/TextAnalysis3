{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Twitter data- Capitol Hill Riots on January 6, 2021\n",
    "\n",
    "\n",
    "#### Description\n",
    "\n",
    "This notebook explores a Twitter dataset containing Tweets sent during the Capitol Hill riots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data exploration\n",
    "Before we can begin exploring the Twitter data, we will install a few libraries designed to work with text. You will only need to pip install these libraries once. After the install, it is recommended to comment out these install lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textblob is a basic API for common natural language processing (NLP) tasks \n",
    "# such as part-of-speech tagging, noun phrase extraction, sentiment analysis, \n",
    "# classification, translation, and more. Learn more here https://textblob.readthedocs.io/en/dev/\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The wordcloud package will help us to make wordclouds to better display keywords in the discourse\n",
    "# You can read more on wordcloud here https://pypi.org/project/wordcloud/\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#langdetect is a language detection library ported from Google.\n",
    "#more information is available here https://pypi.org/project/langdetect/\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code installs libraries that we will need to import below for our analysis of the Twitter data. We will collect tweets during the Capitol Hill Riots and use these libraries to assist in our analysis. Remember, we must first import the libraries we need before we can begin analyzing or plotting. We are importing more libraries than just what was installed above. We will also import pandas (to work with data in a data frame), numpy (to perform basic matrix transformations), re (to work with regular expressions for strings), string (for common string operations such as concatenation, pil (for python images), nltk (for sentiment analysis), sklearn (for feature extraction and ml), and matplotlib as well as plotly (for visualizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text processing\n",
    "import re \n",
    "from textblob import TextBlob\n",
    "import string\n",
    "\n",
    "# Word cloud visualization\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "\n",
    "# Machine learning (sentiment analysis)\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# This one will be used to help us with lexicon\n",
    "import nltk\n",
    "\n",
    "# Other visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"tcat_Jan6th_Proud_Boys-20210106-20210106------------fullExport--9654fe3ff4.csv\"\n",
    "twitter_data = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates, inplace allows us to overwrite the data in memory\n",
    "twitter_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the first five entries in our data with no duplicates\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove a few columns \n",
    "twitter_data = twitter_data.drop(['withheld_copyright', 'withheld_scope', 'truncated', 'lat', 'lng',\n",
    "                                 'from_user_utcoffset', 'from_user_timezone', 'from_user_lang', \n",
    "                                 'from_user_withheld_scope'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda fucntion: apply the same operation on a given subset of the data\n",
    "\n",
    "A lambda function is a small function containing a single expression. Lambda functions can also act as anonymous functions where they donâ€™t require any name. These are very helpful when we have to perform small tasks with less code.\n",
    "\n",
    "Lambda functions consist of three parts: for ex:\n",
    "\n",
    "lambda x: re.sub(\"RT @\\w+: \",\"\",x)\n",
    "\n",
    "-Keyword (lambda)\n",
    "-Bound variable/argument, and (x)\n",
    "-Body or expression (re.sub(\"RT @\\w+: \",\"\",x))\n",
    "The keyword is mandatory, and it must be a lambda, whereas the arguments and body can change based on the requirements.\n",
    "\n",
    "regular expressions: we use the \"re\" (stands for 'regular expression') library to build an expression that reflects patterns in the text we want to find. More info on re: https://docs.python.org/3/library/re.html\n",
    "\n",
    "The text is RT @some_twitter_user some user could be a combination of numbers, letters\n",
    "\n",
    "helpful site: https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do some regular expressions\n",
    "# regular expressions -\n",
    "# lambda: apply the same operation on a given subset of the data, you get to define what that function is\n",
    "#RT @someletters:\n",
    "remove_rt = lambda x: re.sub(\"RT @\\w+: \",\"\",x)\n",
    "rt = lambda x: re.sub('(@[A-Za-z0-9]+)',\" \",x)\n",
    "twitter_data['text'] = twitter_data['text'].map(remove_rt).map(rt)\n",
    "twitter_data['text'] = twitter_data['text'].str.lower()\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Machine learning modelling\n",
    "\n",
    "Natural language processing (sentiment analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores in SentimentIntensityAnalyzer are negative, neutral, positive, compound\n",
    "\n",
    "text_test = \"Hahahaha that is funny\"\n",
    "\n",
    "score = SentimentIntensityAnalyzer().polarity_scores(text_test)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Compound score is a metric that calculates the sum of all the lexicon ratings which\n",
    "#have been normalized \n",
    "#between -1(most extreme negative) and +1 (most extreme positive).\n",
    "\n",
    "#positive sentiment : (compound score >= 0.05)\n",
    "#neutral sentiment : (compound score > -0.05) and (compound score < 0.05)\n",
    "#negative sentiment : (compound score <= -0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a couple new columns: polarity and subjectivity\n",
    "# polarity is a range between (-1, 1). 1 is a positive statement, -1 is a negative statement\n",
    "# subjectivity refers to personal opinion, emotion or judgement, whereas objective refers to fact\n",
    "# subjectivity ranges from (0,1), where 0 is pure personal emotion, 1 is known fact\n",
    "twitter_data[['polarity','subjectivity']] = twitter_data['text'].apply(lambda Text : pd.Series(TextBlob(Text).sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing a score for the text column using SentimentIntensityAnalyzer \n",
    "# If you have a \"lexicon error\", try the following\n",
    "nltk.download('vader_lexicon')\n",
    "for index,row in twitter_data['text'].iteritems():\n",
    "    # compute a score\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    # Assign score categories to variables\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    \n",
    "    # If negative score (neg) is greater than positive score (pos), then the text should be categorized as \"negative\"\n",
    "    if neg> pos:\n",
    "        twitter_data.loc[index,\"sentiment\"] = 'negative'\n",
    "    # If positive score (pos) is greater than the negative score (neg), then the text should be categorized as \"positive\"\n",
    "    elif pos > neg:\n",
    "        twitter_data.loc[index,\"sentiment\"] = \"positive\"\n",
    "    # Otherwise \n",
    "    else:\n",
    "        twitter_data.loc[index,\"sentiment\"] = \"neutral\"\n",
    "        twitter_data.loc[index,'neg'] = neg\n",
    "        twitter_data.loc[index,'pos'] = pos\n",
    "        twitter_data.loc[index,'neu'] = neu\n",
    "        twitter_data.loc[index,'compound'] = comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go back in to our data set and look specifically at the variable sentiment. What are the unique values?\n",
    "twitter_data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after you ran the above statement, you will get a new sentiment column for each tweet, plus\n",
    "#a column for each neg, pos, neu, compound score\n",
    "twitter_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Data visualizing \n",
    "In this section we want to understand the polarity and subjectivity of the tweets in our sample in a visual format. This will give us the ability to summarize thousands of Tweets in a more meaningful representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how many are labelled positive, negative or neutral\n",
    "tw_list_negative = twitter_data[twitter_data['sentiment']=='negative']\n",
    "tw_list_positive = twitter_data[twitter_data['sentiment']=='positive']\n",
    "tw_list_neutral = twitter_data[twitter_data['sentiment']=='neutral']\n",
    "\n",
    "# Let's count how many of these values belong to each category. We will define a function to count values.\n",
    "def count_values_in_column(data,feature):\n",
    "    \n",
    "    total = data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage = round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    \n",
    "    return pd.concat([total,percentage],axis=1, keys=['Total', 'Percentage'])\n",
    "\n",
    "# Values for sentiment\n",
    "pc = count_values_in_column(twitter_data, \"sentiment\")\n",
    "\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a piechart\n",
    "names = pc.index\n",
    "size = pc['Percentage']\n",
    "my_circle = plt.Circle((0,0), 0.5, color='white')\n",
    "plt.pie(size, labels=names,colors=['blue','red','gold'])\n",
    "p = plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Create Wordcloud\n",
    "def create_wordcloud(text):\n",
    "    mask = np.array(Image.open(\"cloud.jpeg\"))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "    mask = mask,\n",
    "    max_words=3000,\n",
    "    stopwords=stopwords,\n",
    "    repeat=True)\n",
    "    wc.generate(str(text))\n",
    "    wc.to_file(\"wc.png\")\n",
    "    print(\"Word Cloud Saved Successfully\")\n",
    "    path=\"wc.png\"\n",
    "    display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for all tweets\n",
    "create_wordcloud(twitter_data[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for positive sentiment\n",
    "create_wordcloud(tw_list_positive[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for negative sentiment\n",
    "create_wordcloud(tw_list_negative[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tweetâ€™s length and word count\n",
    "twitter_data['text_len'] = twitter_data['text'].astype(str).apply(len)\n",
    "twitter_data['text_word_count'] = twitter_data['text'].apply(lambda x: len(str(x).split()))\n",
    "round(pd.DataFrame(twitter_data.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(twitter_data.groupby(\"sentiment\").text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0â€“9]+', '', text)\n",
    "    return text\n",
    "twitter_data['punct'] = twitter_data['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#Applying tokenization- splitting a phrase, sentence, paragraph, or an entire text document into smaller units\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "twitter_data['tokenized'] = twitter_data['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "twitter_data['nonstop'] = twitter_data['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#Applying Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "twitter_data['stemmed'] = twitter_data['nonstop'].apply(lambda x: stemming(x))\n",
    "\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(twitter_data['text'])\n",
    "print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "#print(countVectorizer.get_feature_names())\n",
    "\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum(),columns=[\"Value\"])\n",
    "countdf = count.sort_values(\"Value\",ascending=False).head(20)\n",
    "\n",
    "px.bar(countdf[1:],x=countdf.index[1:],y=\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tw_list_positive.loc[0,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = SentimentIntensityAnalyzer().polarity_scores(tw_list_positive.loc[0,'text'])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something in the lexicon may be triggering an overwhelming positivity on the sentiment assessment of these tweets. Let's remove the term \"Proud Boys\" and see how the classification performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda function to remove \"Proud Boys\"\n",
    "remove_pb = lambda x: re.sub(\"proud boys\",\"\",x)\n",
    "\n",
    "twitter_data['text'] = twitter_data['text'].map(remove_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reclassify\n",
    "\n",
    "#Computing a score using sentiment analyzer\n",
    "\n",
    "for index,row in twitter_data['text'].iteritems():\n",
    "    #compute a score\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    \n",
    "    #Assign score categories to variables\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score ['compound']\n",
    "    \n",
    "    #if negative score(neg) is greater than positive score (pos), then the text should be \"negative\"\n",
    "    \n",
    "    if neg > pos:\n",
    "        twitter_data.loc[index,\"sentiment\"] = 'negative'\n",
    "    elif pos > neg:\n",
    "        twitter_data.loc[index,\"sentiment\"] = 'positive'\n",
    "    else:\n",
    "        twitter_data.loc[index,\"sentiment\"] = \"neutral\"\n",
    "        twitter_data.loc[index,'neg'] = neg\n",
    "        twitter_data.loc[index,'pos'] = pos\n",
    "        twitter_data.loc[index,'neu'] = neu\n",
    "        twitter_data.loc[index,'compound'] = comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize again\n",
    "\n",
    "#let's take a look at how many are labelelled positive, negative or neutral\n",
    "\n",
    "tw_list_negative = twitter_data[twitter_data['sentiment']=='negative']\n",
    "\n",
    "tw_list_positive = twitter_data[twitter_data['sentiment']=='positive']\n",
    "\n",
    "tw_list_neutral = twitter_data[twitter_data['sentiment']=='neutral']\n",
    "\n",
    "#count how many values belong to each category (how many tweets are positive, negative, neutral)\n",
    "\n",
    "def count_values_in_column(data,feature):\n",
    "    total = data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage = round(data.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2)\n",
    "    \n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "\n",
    "pc = count_values_in_column(twitter_data, \"sentiment\")\n",
    "\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 'LMAOOOOOOOO' from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda function to remove \"LMAOOOOOOOO\"\n",
    "remove_pb = lambda x: re.sub(\"lmaoooooooo\",\"\",x)\n",
    "\n",
    "twitter_data['text'] = twitter_data['text'].map(remove_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reclassify\n",
    "\n",
    "#Computing a score using sentiment analyzer\n",
    "\n",
    "for index,row in twitter_data['text'].iteritems():\n",
    "    #compute a score\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    \n",
    "    #Assign score categories to variables\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score ['compound']\n",
    "    \n",
    "    #if negative score(neg) is greater than positive score (pos), then the text should be \"negative\"\n",
    "    \n",
    "    if neg > pos:\n",
    "        twitter_data.loc[index,\"sentiment\"] = 'negative'\n",
    "    elif pos > neg:\n",
    "        twitter_data.loc[index,\"sentiment\"] = 'positive'\n",
    "    else:\n",
    "        twitter_data.loc[index,\"sentiment\"] = \"neutral\"\n",
    "        twitter_data.loc[index,'neg'] = neg\n",
    "        twitter_data.loc[index,'pos'] = pos\n",
    "        twitter_data.loc[index,'neu'] = neu\n",
    "        twitter_data.loc[index,'compound'] = comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize again\n",
    "\n",
    "#let's take a look at how many are labelelled positive, negative or neutral\n",
    "\n",
    "tw_list_negative = twitter_data[twitter_data['sentiment']=='negative']\n",
    "\n",
    "tw_list_positive = twitter_data[twitter_data['sentiment']=='positive']\n",
    "\n",
    "tw_list_neutral = twitter_data[twitter_data['sentiment']=='neutral']\n",
    "\n",
    "#count how many values belong to each category (how many tweets are positive, negative, neutral)\n",
    "\n",
    "def count_values_in_column(data,feature):\n",
    "    total = data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage = round(data.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2)\n",
    "    \n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "\n",
    "pc = count_values_in_column(twitter_data, \"sentiment\")\n",
    "\n",
    "pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5 Conclusions\n",
    "\n",
    "Refer back to our first table made under section 4 visualization - the majority of tweets (86.89%) of the tweets were labelled as positive, whereas only 12.37% were labelled as negative. We find word clouds that share similar patterns with prominent display of terms like \"proud boys\", \"capitol\" and \"national guard\". The most prominent terms include \"boy\", \"stay\", \"trump\", \"people\". Why are the tweets so overwhelmingly positive? In the final section we explore the removal of key terms. When we cut references to \"proud boys\" and \"lmaooo\" we see a shift in the overall sentiment analysis. This highlights a few key limitations in NLTK. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
